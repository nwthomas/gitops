# Based on official otwld/ollama-helm chart with custom configurations

replicaCount: 1

image:
  repository: ollama/ollama
  tag: "0.12.10"
  pullPolicy: Always

ollama:
  port: 11434
  gpu:
    enabled: true
    type: "nvidia"
    number: 1
    nvidiaResource: "nvidia.com/gpu"
  models:
    pull:
      - gpt-oss:20b
      - qwen2.5vl:7b
      - qwen3-coder:30b
      - qwen3:30b
      - MichelRosselli/GLM-4.5-Air:Q2_K
    create:
      - name: gpt-oss:20b-131k-context
        template: |
          FROM gpt-oss:20b
          PARAMETER num_ctx 131072
      - name: qwen3-coder:30b-32k-context
        template: |
          FROM qwen3-coder:30b
          PARAMETER num_ctx 32768
      - name: qwen3:30b-32k-context
        template: |
          FROM qwen3:30b
          PARAMETER num_ctx 32768
      - name: GLM-4.5-Air-128k-context
        template: |
          FROM MichelRosselli/GLM-4.5-Air:Q2_K
          PARAMETER num_ctx 128000
  insecure: false
  mountPath: ""

serviceAccount:
  create: true
  automount: true
  annotations: {}
  name: ""

rbac:
  create: true

sealedSecret:
  create: true
  encryptedData:
    loadBalancerIP: "AgBDEhn10OPXJ/eEAns/sPjgRwNVXXp9l83pDfcEJcMsQmpJZBfnBZfDQC9uoKANzzqrW9OA8rUz416Mc+kU+7veHHN0UX1RBYDRSqAK1LWf4mTa3s2tAt3f4433ZXsc/7w0UHYE6FP9fkOjGqMs5XA+QQkuSPQVM/qYq67ZQ79Urmp4zzxWojKHTPhcPnYcUXQHpLpVC9+R42dBqxCNp+m525e3RNPMwbjW5uL4NszOTP8/yZEbsQgiglmeBzXHVUhwUiegDk8wqS9jEsCIwZJzyE4MUKz0TKTsZXLVb2qzcaX1DGUX4VbOB2jHJ9gNJccZtbvlrlevWi9CRCfsFR+5au0k2E8pisOAMl13hxEuX+Rg3kLN7NqKK9TrgUFbSqzn45ESHvq9Ev/j/1AUOfv4QJnx7O4Vpt+XWoQPM8H3cxkABxuibMZw7aeINb1ZrrVVImY7r25knGCoghdM30iSCqlrw64UIf1/dZB9DbQB96oV9qCmWAbTaRJAMlQ8p5nte04Wf2bkr7GmYJCsqzK7/BrA63tLGeL1TSbriOL+H12G8XmRxksB6VjGw/8zTQ0pejCh2cv3fP9ewMCjlO18xtzdPHZNBrxm4J6r1/cvOfT21j43FcNZc7mmQrbU0dEWkS00tEDBZOJsAh+XX0RMrB8uOBiXvjVFEXSfcEJpi6uvkWans0rNV25LJ08ATiVnoELsW0YCkNIg"

patchJob:
  create: true

podAnnotations: {}

podLabels: {}

podSecurityContext: {}

priorityClassName: ""

securityContext:
  runAsNonRoot: false
  runAsUser: 0

runtimeClassName: nvidia

service:
  type: LoadBalancer
  port: 11434
  nodePort: 31434
  loadBalancerIP: ""
  annotations: {}
  labels: {}

deployment:
  labels: {}

ingress:
  enabled: false
  className: ""
  annotations: {}
  hosts:
    - host: ollama.local
      paths:
        - path: /
          pathType: Prefix
  tls: []

resources:
  requests:
    nvidia.com/gpu: 1
    memory: "100Gi"
    cpu: "4"
  limits:
    nvidia.com/gpu: 1
    memory: "100Gi"
    cpu: "8"

livenessProbe:
  enabled: true
  path: /
  initialDelaySeconds: 300
  periodSeconds: 30
  timeoutSeconds: 10
  failureThreshold: 10
  successThreshold: 1

readinessProbe:
  enabled: true
  path: /
  initialDelaySeconds: 180
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 10
  successThreshold: 1

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80

volumes: []

volumeMounts: []

extraArgs: []

# See this file for list of available extraEnv: https://github.com/ollama/ollama/blob/main/envconfig/config.go
extraEnv:
  - name: OLLAMA_CONTEXT_LENGTH
    value: "131072"
  # Change to "1" for verbose logging with more error logs and "0" for normal logging
  - name: OLLAMA_DEBUG
    value: "1"
  - name: OLLAMA_FLASH_ATTENTION
    value: "true"
  - name: OLLAMA_HOST
    value: "http://0.0.0.0:11434"
  - name: OLLAMA_KEEP_ALIVE
    value: "60m"
  # This is a critial value - "cuda" is not sufficient, but version is needed
  - name: OLLAMA_LLM_LIBRARY
    value: "cuda_v13"
  - name: OLLAMA_LOAD_TIMEOUT
    value: "2m"
  - name: OLLAMA_NUM_PARALLEL
    value: "1"
  - name: OLLAMA_USE_GPU
    value: "true"
  - name: NVIDIA_DRIVER_CAPABILITIES
    value: "compute,utility"
  - name: LD_LIBRARY_PATH
    value: "/usr/lib/ollama:/usr/lib/ollama/cuda_v13:/usr/lib/x86_64-linux-gnu"
  - name: CUDA_VISIBLE_DEVICES
    value: "0"

extraEnvFrom: []

persistentVolume:
  enabled: true
  accessModes:
    - ReadWriteOnce
  annotations:
    # Set the number of replicas for this Longhorn volume by overriding the storage class default
    longhorn.io/number-of-replicas: "1"
  existingClaim: ""
  size: 200Gi
  storageClass: "longhorn"
  volumeMode: ""
  subPath: ""
  volumeName: ""

nodeSelector:
  kubernetes.io/arch: amd64
  gpu: "true"

tolerations: []

affinity: {}

lifecycle: {}

updateStrategy:
  type: "Recreate"

topologySpreadConstraints: {}

terminationGracePeriodSeconds: 120

initContainers: []

hostIPC: false

hostPID: false

hostNetwork: false

extraObjects: []

tests:
  enabled: true
  labels: {}
  annotations: {}